import os, json, praw, pandas as pd
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()
reddit = praw.Reddit(
    client_id=os.getenv("REDDIT_CLIENT_ID"),
    client_secret=os.getenv("REDDIT_SECRET"),
    user_agent=os.getenv("USER_AGENT"))

# Load sale events
with open("data/raw/sale_events.json") as f:
    sales = json.load(f)

# Load subreddits
with open("scraper/subreddits.txt") as f:
    subreddits = [line.strip() for line in f if line.strip()]

# Loop through all sales
for selected_sale in sales:
    slug = selected_sale["name"].lower().replace(" ", "_").replace("‚Äô", "").replace("'", "")
    start_date = datetime.strptime(selected_sale["start_date"], "%Y-%m-%d").timestamp()
    end_date = datetime.strptime(selected_sale["end_date"], "%Y-%m-%d").timestamp()

    product_file = f"data/raw/products_{slug}.csv"
    output_file = f"data/raw/{slug}_reddit_comments.csv"

    if not os.path.exists(product_file):
        print(f"‚ö†Ô∏è Skipping {selected_sale['name']} ‚Äî product file not found.")
        continue

    products_df = pd.read_csv(product_file)
    product_names = [p.lower() for p in products_df["product_name"].tolist()]
    results = []

    for sub in subreddits:
        try:
            subreddit = reddit.subreddit(sub)
            subreddit.id
            print(f"üîç Searching r/{sub} for {selected_sale['name']}...")
        except Exception as e:
            print(f"‚ö†Ô∏è Skipping invalid subreddit: r/{sub} ‚Üí {e}")
            continue

        for submission in subreddit.search("Amazon", limit=10):
            if start_date <= submission.created_utc <= end_date:
                submission.comments.replace_more(limit=0)
                for comment in submission.comments:
                    if hasattr(comment, "body"):
                        body = comment.body.lower()
                        for product in product_names:
                            if product in body:
                                results.append({
                                    "subreddit": sub,
                                    "product": product,
                                    "comment": comment.body,
                                    "created_utc": comment.created_utc,
                                    "submission_title": submission.title,
                                    "url": submission.url
                                })

    if results:
        os.makedirs("data/raw/", exist_ok=True)
        output_df = pd.DataFrame(results)
        output_df.to_csv(output_file, index=False)
        print(f"‚úÖ {selected_sale['name']} ‚Äî Scraped and saved to {output_file}")
    else:
        print(f"‚ÑπÔ∏è No comments found for {selected_sale['name']}")


  # Previous Code 

# import praw  # praw is a reddit api scraper using python 
# import pandas as pd
# import json
# from datetime import datetime
# from dotenv import load_dotenv
# import os
# import csv 

# load_dotenv() # load environment variables 

# # Initialize Reddit API
# reddit = praw.Reddit(
#     client_id=os.getenv("REDDIT_CLIENT_ID"),
#     client_secret=os.getenv("REDDIT_SECRET"),
#     user_agent=os.getenv("USER_AGENT")
# )

# # Load sale events
# with open("data/sale_events.json") as f:
#     sales = json.load(f)

# # Choose the sale you want to scrape
# selected_sale = next(s for s in sales if s["name"] == "Fab TV Sale")
# start_date = datetime.strptime(selected_sale["start_date"], "%Y-%m-%d").timestamp()
# end_date = datetime.strptime(selected_sale["end_date"], "%Y-%m-%d").timestamp()

# # Load product names
# products_df = pd.read_csv("data/products_fab_tv_sale.csv")
# product_names = [p.lower() for p in products_df["product_name"].tolist()]

# # Load subreddits
# with open("scraper/subreddits.txt") as f:
#     subreddits = [line.strip() for line in f if line.strip()]

# # Result storage
# results = []

# # Scrape posts/comments
# # for sub in subreddits:
# #     subreddit = reddit.subreddit(sub)
# #     print(f"Searching r/{sub}...")

# for sub in subreddits:
#     try:
#         subreddit = reddit.subreddit(sub)
#         subreddit.id  # Forces PRAW to fetch subreddit, raises error if invalid
#         print(f"üîç Searching r/{sub}...")
#     except Exception as e:
#         print(f"‚ö†Ô∏è Skipping invalid subreddit: r/{sub} ‚Üí {e}")
#         continue

#     for submission in subreddit.search("Amazon", limit=10):
#         # print("üîó Found post:", submission.title)
#         if start_date <= submission.created_utc <= end_date:
#             submission.comments.replace_more(limit=0)
#             for comment in submission.comments.list():
#                 # print("üó®Ô∏è", comment.body[:100])
#                 if hasattr(comment, "body"):
#                     body = comment.body.lower()
#                     for product in product_names:
#                         print("üì¶ Products to match:", product_names[:5])# print the 1st 5 values of the product name 
#                         if product.split()[0] in body:
#                             print(f"üìå MATCHED ‚Üí Product: {product} | Comment: {comment.body[:100]}")
#                             results.append({
#                                 "subreddit": sub,
#                                 "product": product,
#                                 "comment": comment.body,
#                                 "created_utc": comment.created_utc,
#                                 "submission_title": submission.title,
#                                 "url": submission.url
#                             })

# # to make sure the data/raw/ folder exists befor adding 
# os.makedirs("data/raw/", exist_ok=True)

# # Save to CSV
# # with open("data/raw/fab_tv_sale_reddit_comments.csv", "w", newline="", encoding="utf-8") as f:
# #     writer = csv.writer(f)
# #     writer.writerow(["comment", "score", "timestamp"]) # columns / headers 
# print(f"üßÆ Total matches found: {len(results)}")

# if not results:
#     print("‚ùå No matching comments found. CSV not written.")
#     exit()
    
# if results:
#     output_df = pd.DataFrame(results)
#     output_df.to_csv("data/raw/fab_tv_sale_reddit_comments.csv", index=False)
#     print(f"‚úÖ Scraping done. Data saved to fab_tv_sale_reddit_comments.csv")
# else:
#     print("‚ùå No matching comments found for products.")
